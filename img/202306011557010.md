### 什么是爬虫？

按照一定的规则，自动地抓取互联网信息的程序。

### 网页数据类型

html, json, xml...

### 爬虫步骤

1、找数据（需要什么数据？文本、图片、视频、音频...）

2、爬取数据（spider）

3、解析数据（bs4，parsel）

4、保存数据（file.write（））

### Requests库

*优点：简单*

1. 
2. o
3. 

### 环境安装

```python
pip install requests
```

### 爬虫程序

#### 豆瓣

```python
import csv

import requests

url = 'https://movie.douban.com/j/chart/top_list?'

headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/112.0.0.0 Safari/537.36',
}

param = {
    'type_name': '动作',
    'type': 5,
    'interval_id': '100:90',
    'action': '',
    'start': 0,
    'limit': 20
}

def get_data(url, headers,param):
    response = requests.get(url=url, headers=headers, params=param)
    json_data = response.json()
    return json_data


def save_data(json_data):
    with open('movie.csv', mode='w', encoding='utf-8', newline='') as f:
        wr = csv.writer(f, delimiter=',')
        wr.writerow(['电影名', '上映时间', '类型', '链接地址', '地区', '演员', '评分', '评价人数'])
        for i in range(len(json_data)):
            name = json_data[i]['title']
            release_date = json_data[i]['release_date']
            url_ = json_data[i]['url']
            regions = json_data[i]['regions']
            types_ = json_data[i]['types']
            actors = json_data[i]['actors']
            score = json_data[i]['score']
            vote_count = json_data[i]['vote_count']
            wr.writerow([name, release_date, types_, url_, regions, actors, score, vote_count])
            print(name, url_, regions, types_, actors)


if __name__ == '__main__':
    js = get_data(url,headers, param)
    save_data(js)
```

#### 国家统计网

```python
import csv
import json

import requests

url = 'https://data.stats.gov.cn/search.htm'

headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/112.0.0.0 Safari/537.36'}


def page(i):
    params = {
        's': '2000年-2010年gdp',
        'm': 'searchdata',
        'db': '',
        'p': i
    }
    return params


def get_data(url, headers, params):
    response = requests.get(url=url, headers=headers, params=params, verify=False)
    return json.loads(response.text)


def parse_data(json_data):
    info = []
    for i in range(10):
        name = json_data['result'][i]['zb']
        reg = json_data['result'][i]['reg']
        data = json_data['result'][i]['data']
        date_ = json_data['result'][i]['sj']
        db = json_data['result'][i]['db']
        info.append([name, reg, data, date_, db])
    return info


def save_data(info_list):
    with open('govn.csv', mode='w', encoding='utf8', newline='') as f:
        w = csv.writer(f, delimiter=',')
        w.writerow(['指标', '地区', '数值', '时间', '所属栏目'])
        for i in range(len(info_list)):
            w.writerow(info_list[i])
            print(info_list[i])


if __name__ == '__main__':
    for i in range(2000):
        save_data(parse_data(get_data(url, headers, page(i))))

```

#### 豆瓣top10

```python
import csv
import re

# lxml库更新后需要使用该语句导入etree功能
import lxml.html
etree = lxml.html.etree 

import parsel
import requests

url = 'https://movie.douban.com/chart'

headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/112.0.0.0 Safari/537.36'}


def get_data(url, headers, *args):
    response = requests.get(url=url, headers=headers)
    return response.text


def parse_data(data):
    info = []
    se = parsel.Selector(data)
    title = se.css('.nbg::attr(title)').getall()
    inf = se.css('td > div > p::text').getall()
    for i in range(len(title)):
        info.append([title[i], inf[i]])
    return info


def parse_data_2(data):
    html = etree.HTML(data)
    title = html.xpath('//td/div/a/text()')
    inf = html.xpath('//td//div//p/text()')
    info = []
    for i in range(0, len(title), 2):
        info.append(
            title[i].replace('\n                        ', '').replace('\n                        / ', '').replace('/',
                                                                                                                   ''))
    # print(info)
    return info


def save_data(info_list):
    with open('top10.csv', mode='w', encoding='utf8', newline='') as f:
        w = csv.writer(f, delimiter=',')
        w.writerow(['电影名', '电影信息'])
        for i in range(len(info_list)):
            w.writerow(info_list[i])


if __name__ == '__main__':
    save_data(parse_data(get_data(url, headers)))
    save_data(parse_data_2(get_data(url, headers)))

```

#### 历年文科高考分数线

```python
import csv
import re

import lxml.html

etree = lxml.html.etree

import parsel
import requests

url = 'https://www.gaokao.com/sichuan/scgkcj/'

headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/112.0.0.0 Safari/537.36'}


def get_data(url, headers, *args):
    response = requests.get(url=url, headers=headers)
    response.encoding = response.apparent_encoding
    return response.text


def parse_data(data):
    info = []
    se = parsel.Selector(data)
    w_yiben = se.css('.c_blue > .first::text').get()
    w_erben = se.css('.c_white > .first::text').get()
    year = se.css(' .wkTit > th::text').getall()[:11]
    first_ = se.css(' .c_blue > td::text').getall()

    w_first = first_[1:12]
    second_ = se.css('.c_white > td::text').getall()
    w_second = second_[1:12]

    for i in range(len(w_first)):
        info.append((year[i], w_yiben, w_first[i]))
        info.append((year[i], w_erben, w_second[i]))
    return info


def parse_data_2(data):
    html = etree.HTML(data)
    info = []
    w_yiben = html.xpath('//tr[@class="c_blue"]/td[@class="first"]/text()')[0]
    w_year = html.xpath('//tr[@class="wkTit"]/th/text()')[:11]
    w_erben = html.xpath('//tr[@class="c_white"]/td[1]/text()')[0]
    first_ = html.xpath('//tr[@class="c_blue"]/td/text()')[1:12]
    second_ = html.xpath('//tr[@class="c_white"]/td/text()')[1:12]
    for i in range(len(first_)):
        info.append((w_year[i], w_yiben, first_[i]))
        info.append((w_year[i], w_erben, second_[i]))
    return info


def save_data(info_list):
    with open('历年文科.csv', mode='w', encoding='utf8', newline='') as f:
        w = csv.writer(f, delimiter=',')
        w.writerow(['年份', '类别', '分数'])
        for i in range(len(info_list)):
            w.writerow(info_list[i])


if __name__ == '__main__':
    # xpath
    print(parse_data_2(get_data(url, headers)))
    # css
    print(parse_data(get_data(url, headers)))
    save_data(parse_data(get_data(url, headers)))
```

#### 王者荣耀英雄

```python
import requests

url = 'https://pvp.qq.com/web201605/js/herolist.json'

headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/112.0.0.0 Safari/537.36'}


def get_data(url, headers, *args):
    response = requests.get(url=url, headers=headers)
    response.encoding = response.apparent_encoding
    return response.json()


def parse_data(data):
    print(len(data))
    for i in range(len(data)):
        print(data[i]['cname'])


if __name__ == '__main__':
    parse_data(get_data(url, headers))

```

#### 2017-2019年云南、四川、重庆的人口出生率

```python
import csv

import re

import requests

url = 'https://data.stats.gov.cn/search.htm?'
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/112.0.0.0 Safari/537.36'}


def page(i):
    params = {
        's': '2017-2019年云南、四川、重庆的人口出生率',
        'm': 'searchdata',
        'db': '',
        'p': i
    }
    return params


def get_data(url, headers, params):
    response = requests.get(url=url, headers=headers, params=params, verify=False)
    response.encoding = response.apparent_encoding
    return response.text


def parse_data(data):
    info = []
    data_ = re.findall('data":"(.*?)","', data, re.S)
    db = re.findall('db":"(.*?)","exp', data, re.S)
    rank = re.findall('"rank":(.*?),"reg', data, re.S)
    reg = re.findall('reg":"(.*?)","report', data, re.S)
    sj = re.findall('sj":"(.*?)","zb', data, re.S)
    zb = re.findall('zb":"(.*?)"}', data, re.S)
    for i in range(len(zb)):
        info.append([data_[i], db[i], rank[i], reg[i], sj[i], zb[i]])
    return info


def save_data(info_list):
    with open('birthrate.csv', mode='a', encoding='utf8', newline='') as f:
        w = csv.writer(f, delimiter=',')
        # w.writerow(['数值', '所属栏目', '排名', '地区', '时间', '指标'])
        for i in range(len(info_list)):
            w.writerow(info_list[i])
            print(info_list[i])


if __name__ == '__main__':
    # print(get_data(url, headers, page(0)))
    for i in range(200):
        save_data(parse_data(get_data(url, headers, page(i))))

```

